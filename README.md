# Machine-Learning-Projects

The repository contains Machine Learning projects developed by me in Python using Jupyter notebook, for learning and reference purposes. All these projects were developed based on the template I created for Machine Learning projects. The details of the projects are given below.

### **MLP1-Classification-RandomForest-DiabetesDetection**   
`Name of the notebook:` MLPy-DemoProj-1.ipynb   
`Type of the problem:` Supervised Learning - Classification - Univariate   
`Data source(s):` [Kaggle](https://www.kaggle.com/)   
`Libraries used:`
```
1) Numpy
2) Pandas
3) Sklearn
4) Seaborn
5) Matplotlib
6) Pickle
```
`Algorithms used to build models:`
```
1) Logistic Regression (LR)      - Simple Linear
2) SGD Classifier (SGD)          - Simple Linear
3) K-Nearest Neighbors (KNN)     - Nonlinear
4) Support Vector Machines (SVM) - Nonlinear
5) Gaussian Naive Bayes (NB)     - Nonlinear
6) Decision Trees (DT)           - Nonlinear
7) Random Forest (RF)            - Ensemble Bagging
8) Gradient Boosting (GB)        - Ensemble Boosting   
9) Extreme Boosting (XGB)        - Ensemble Boosting
```
`Metrics used to evaluate the model performances:`
```
1) Accuracy score
2) Confusion matrix (not a metric)
3) Classification report (not a metric)
```
`Winning algorithm:` Random Forest (RF)


### **MLP2-Classification-LogisticRegression-PlantIdentification**
`Name of the notebook:` MLPy-DemoProj-2.ipynb   
`Type of the problem:` Supervised Learning - Classification - Multivariate   
`Data source(s):` [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets.php)   
`Libraries used:`
```
1) Numpy
2) Pandas
3) Sklearn
4) Seaborn
5) Imblearn
6) Matplotlib
7) Pickle
```
`Algorithms used to build models:`
```
1) Logistic Regression (LR)      - Simple Linear
2) SGD Classifier (SGD)          - Simple Linear
3) K-Nearest Neighbors (KNN)     - Nonlinear
4) Support Vector Machines (SVM) - Nonlinear
5) Gaussian Naive Bayes (NB)     - Nonlinear
6) Decision Trees (DT)           - Nonlinear
7) Random Forest (RF)            - Ensemble Bagging
8) Gradient Boosting (GB)        - Ensemble Boosting   
9) Extreme Boosting (XGB)        - Ensemble Boosting
```
`Metrics used to evaluate the model performances:`
```
1) Accuracy score
2) Confusion matrix (not a metric)
3) Classification report (not a metric)
```
`Winning algorithm:` Logistic Regression (LR)


### **MLP3-Classification-RandomForest-MushroomIdentification**
`Name of the notebook:` MLPy-DemoProj-3.ipynb   
`Type of the problem:` Supervised Learning - Classification - Univariate - Imbalanced   
`Data source(s):` UCI Machine Learning Repository - Mushroom dataset   
`Libraries used:`
```
1) Numpy
2) Pandas
3) Sklearn
4) Seaborn
5) Matplotlib
6) Pickle
```
`Algorithms used to build models:`
```
 1) Logistic Regression (LR)      - Simple Linear
 2) SGD Classifier (SGD)          - Simple Linear
 3) K-Nearest Neighbors (KNN)     - Nonlinear
 4) Support Vector Machines (SVM) - Nonlinear
 5) Gaussian Naive Bayes (NB      - Nonlinear
 6) Neural Network (NN)           - Nonlinear
 7) Decision Trees (DT)           - Nonlinear
 8) Random Forest (RF)            - Ensemble Bagging
 9) Gradient Boosting (GB)        - Ensemble Boosting   
10) Extreme Boosting (XGB)        - Ensemble Boosting
```
`Metrics used to evaluate the model performances:`
```
1) Accuracy score
```
`Winning algorithm:` Randon Forest (RF)


### **MLP4-Classification-GaussianNB-BlightTicketCompliance**   
`Name of the notebook:` MLPy-DemoProj-4.ipynb   
`Type of the problem:` Supervised Learning - Classification - Probability   
`Data source(s):` [City of Detroit Open Data Portal-Blight Violations](https://data.detroitmi.gov/datasets/blight-violations) and [Coursera](https://www.coursera.org/)   
`Libraries used:`
```
1) Numpy
2) Pandas
3) Seaborn
4) Matplotlib
5) Pickle
```
`Algorithms used to build models:`
```
1) Logistic Regression (LR)	- Simple Linear
2) K-Nearest Neighbors (KNN)	- Nonlinear
3) Gaussian Naive Bayes (NB)	- Nonlinear
4) Neural Network (NN)		- Nonlinear
5) Decision Trees (DT)		- Nonlinear
6) Random Forest (RF)		- Ensemble Bagging
7) Gradient Boosting (GB)	- Ensemble Boosting   
8) Extreme Boosting (XGB)	- Ensemble Boosting
```
`Metrics used to evaluate the model performances:`
```
1) Area Under the ROC curve (AUC)
```
`Winning algorithm:` Gaussian Naive Bayes (NB)


### **MLP5-Regression-OrdinaryLeastSquares-HousePricePrediction**   
`Name of the notebook:` MLPy-DemoProj-5.ipynb   
`Type of the problem:` Supervised Learning - Regression   
`Data source(s):` [Kaggle](https://www.kaggle.com/amitabhajoy/bengaluru-house-price-data)   
`Libraries used:`
```
1) Numpy
2) Pandas
3) Sklearn
4) Seaborn
5) Matplotlib
6) Pickle
```
`Algorithms used to build models:`
```
1) Oridinary Least Squares (OLS) - Linear
2) Lasso                         - Linear
3) Ridge                         - Linear
4) ElasticNet (ENet)             - Linear
5) K-Nearest Neighbors (KNN)     - Non-linear
6) Decision Tree (DT)            - Non-linear
```
`Metrics used to evaluate the model performances:`
```
1) R^2 Score (Coefficient of Determination)
2) MAE (Mean Absolute Error)
3) MSE (Mean Squared Error)
4) Algorithm run-time
```
`Winning algorithm:` Ordinary Least Squares (OLS)


### **MLP6-Classification-ExtremeBoosting-CCCustomerChurn**   
`Name of the notebook:` MLPy-DemoProj-6.ipynb   
`Type of the problem:` Supervised Learning - Classification - Univariate (Imbalanced classification)   
`Data source(s):` [Kaggle](https://www.kaggle.com/sakshigoyal7/credit-card-customers)   
`Libraries used:`
```
1) Numpy
2) Pandas
3) Sklearn
4) SciPy
5) Seaborn
6) Imblearn
7) Matplotlib
8) Pickle
```
`Algorithms used to build models:`
```
 1) Logistic Regression (LR)      - Simple Linear
 2) SGD Classifier (SGD)          - Simple Linear
 3) K-Nearest Neighbors (KNN)     - Nonlinear
 4) Support Vector Machines (SVM) - Nonlinear
 5) Gaussian Naive Bayes (NB)     - Nonlinear
 6) Decision Trees (DT)           - Nonlinear
 7) Random Forest (RF)            - Ensemble Bagging
 8) Gradient Boosting (GB)        - Ensemble Boosting   
 9) AdaBoost (ADA)                - Ensemble Boosting
10) XGBoost  (XGB)                - Ensemble Boosting
```
`Metrics used to evaluate the model performances:`
```
1) CV Scores (Accuracy)
2) ROC-AUC Score
3) Algorithm run-time
4) Confusion matrix (not a metric)
5) Classification report (not a metric)
```
`Winning algorithm:` XGBoost (XGB)


### **MLP7-Clustering-KMeans-CustomerSegmentation**
`Name of the notebook:` MLPy-DemoProj-7.ipynb   
`Type of the problem:` Unsupervised Learning - Clustering   
`Data source(s):` [Kaggle](https://www.kaggle.com/vjchoudhary7/customer-segmentation-tutorial-in-python)   
`Libraries used:`
```
1) Numpy
2) Pandas
3) SciPy
4) Seaborn
5) Matplotlib
6) Pickle
```
`Algorithms used to build models:`
```
1) KMeans Clustering
```
`Metrics used to evaluate the model performances:`
```
1) KMeans score for Elbow method
2) Silhouette score
3) Davies Bouldin score
```


### **MLP8-DA-Clustering-RFMAnalysis-CustomerSegmentation**   
`Name of the notebook:` MLPy-DemoProj-8.ipynb   
`Type of the problem:`
1) Data Analysis - Visualization - for understanding the current sales trend and revenue loss
2) Unsupervised Learning – Clustering – for Customer Segmentation

`Data source(s):` [DataCo SMART SUPPLY CHAIN FOR BIG DATA ANALYSIS](https://data.mendeley.com/datasets/8gx2fvg2k6/5)   
Sitation: *Constante, Fabian; Silva, Fernando; Pereira, António (2019), “DataCo SMART SUPPLY CHAIN FOR BIG DATA ANALYSIS”, Mendeley Data, V5, doi: 10.17632/8gx2fvg2k6.5*

`Libraries used:`
```
1) Numpy
2) Pandas
3) Seaborn
4) Matplotlib
5) Pickle
```
